# **DEEP LEARNING WORK**

This repository contains various deep learning concepts, implementations, and tutorials.

## **1Ô∏è‚É£ Introduction to Deep Learning**
- üîπ [Deep Learning Intro](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/DeepLearningIntro.txt)
- üîπ [Neuron & Neural Network](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Neuron%26NeuralNetwork.ipynb)

## **2Ô∏è‚É£ Perceptron & Multi-Layer Perceptron (MLP)**
- üîπ [Perceptron Implementation](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/PerceptronImplementation.ipynb)
- üîπ [Perceptron Training & Finding 'w' & 'b' Values](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/PerceptronTraining%26Finding'w'%26'b'ValuesInPerceptron.ipynb)
- üîπ [MLP Bias and Weight Notation](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/MultilayerPerceptronNotation.ipynb)
- üîπ [Multilayer Perceptron Training](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/MultilayerPerceptron.ipynb)

## **3Ô∏è‚É£ Forward & Backpropagation**
- üîπ [Forward Propagation](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/ForwardPropogation.ipynb)
- üîπ [Backpropagation - Steps to Update 'w' and 'b'](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Backpropogation.ipynb)
- üîπ [Backpropagation Implementation (without Keras)](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Backpropogation_Implementation.ipynb)

## **4Ô∏è‚É£ Activation & Loss Functions**
- üîπ [Activation Functions Intuition](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/ActivationFunction.ipynb)
- üîπ  [Activation Functions in Depth](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Activation_Functions_In_Deep_Learning.ipynb)
- üîπ  [Dying Relu Problem and Variants of Relu](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/ReLU_Problem_and_Its_Varients.ipynb)
- üîπ [Loss Functions Intuition](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/LossFunctionIntuation.ipynb)
- üîπ [Loss Functions in Depth](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/LossFunctonsInNeuralNetwork.ipynb)

## **5Ô∏è‚É£ Gradient Descent & Optimization**
- üîπ [Gradient Descent in Neural Network](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/GradientDescentInNeuralNetwork.ipynb)
- üîπ [Vanishing Gradient Problem in ANN](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/VanishingGradientProblemInANN.ipynb)
- üîπ [How to Improve Performance of Neural Network](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/HowToImprovePerformanceOfANN.ipynb)

## **6Ô∏è‚É£ Regularization & Optimization Techniques**
- üîπ [Early Stopping in ANN](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Early_Stopping_In_ANN.ipynb)
- üîπ [Feature Scaling & Normalization](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Data_or_Feature_Scaling_Normalization_In_ANN.ipynb)
- üîπ [Dropout Layers in ANN](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Dropout_Layers_In_ANN.ipynb)
- üîπ [Regularization in Deep Learning](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Regularization_In_Deep_Learning.ipynb)

## **7Ô∏è‚É£ ANN Implementations on Real-World Datasets**
- üü¢ **Binary Classification:**  
  - [ANN on Customer Churn Dataset](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/CustomerChurnPredictionUsingANN.ipynb)
- üîµ **Multi-class Classification:**  
  - [ANN on MNIST Dataset](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/MNIST_classification.ipynb)
- üî¥ **Regression Problem:**  
  - [ANN for Regression](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/ANN_For__Regression_Problem.ipynb)
 
## **8Ô∏è‚É£ Weight Initialization Techniques**
- üîπ [What Not To Do When Initializing the Weight Values](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/WeightInitilizationTechnique(WhatNotToDo).ipynb)
- üîπ [Xavier/Glorat And He Weight Initialization Technique](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Xavier_Glorat_And_He_Weight_Initialization_.ipynb)
- 


[Batch Normalization In Deep Learning](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/BatchNormalization_.ipynb)


## **Optimizers:**
- [Optimizers Basics](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Optimizers_.ipynb)
- [Exponentially Weighted Moving Average](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/ExponentiallyWeightedMovingAverage_.ipynb)
- [SGD with Momentum ](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/SGD_with_Momentum_(Optimizers_Part_2).ipynb)
- [Nesterov Accelerated Gradient Descent](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/NesterovAcceleratedGradient(NAG)_Optimizers_Part_3.ipynb)

### **‚ú® How to Use This Repository?**
1Ô∏è‚É£ Browse topics based on categories.  
2Ô∏è‚É£ Click the **"Click Here"** links to open Jupyter Notebooks.  
3Ô∏è‚É£ Run the notebooks on Google Colab or Jupyter Notebook.  

Enjoy learning! üöÄ
