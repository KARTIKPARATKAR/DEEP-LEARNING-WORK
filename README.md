# **üß† DEEP LEARNING WORK**  

This repository contains implementations, tutorials, and resources covering fundamental to advanced deep learning concepts.

---

## **1Ô∏è‚É£ Introduction to Deep Learning**  
- üîπ [Deep Learning Basics](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/DeepLearningIntro.txt)  
- üîπ [Neuron & Neural Network Fundamentals](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Neuron%26NeuralNetwork.ipynb)  

---

## **2Ô∏è‚É£ Perceptron & Multi-Layer Perceptron (MLP)**  
- üîπ [Perceptron Implementation from Scratch](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/PerceptronImplementation.ipynb)  
- üîπ [Training: Finding Optimal Weights ('w') and Bias ('b')](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/PerceptronTraining%26Finding'w'%26'b'ValuesInPerceptron.ipynb)  
- üîπ [MLP Weight & Bias Notation Explained](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/MultilayerPerceptronNotation.ipynb)  
- üîπ [Complete MLP Training Process](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/MultilayerPerceptron.ipynb)  

---

## **3Ô∏è‚É£ Forward & Backpropagation**  
- üîπ [Step-by-Step Forward Propagation](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/ForwardPropogation.ipynb)  
- üîπ [Backpropagation: Weight Update Mechanics](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Backpropogation.ipynb)  
- üîπ [Manual Backpropagation Implementation (No Keras)](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Backpropogation_Implementation.ipynb)  

---

## **4Ô∏è‚É£ Activation & Loss Functions**  
### **Activation Functions**  
- üîπ [Intuition Behind Activation Functions](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/ActivationFunction.ipynb)  
- üîπ [Detailed Analysis of Activation Functions](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Activation_Functions_In_Deep_Learning.ipynb)  
- üîπ [Dying ReLU Problem and Solutions](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/ReLU_Problem_and_Its_Varients.ipynb)  

### **Loss Functions**  
- üîπ [Understanding Loss Functions](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/LossFunctionIntuation.ipynb)  
- üîπ [Comprehensive Guide to Loss Functions](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/LossFunctonsInNeuralNetwork.ipynb)  

---

## **5Ô∏è‚É£ Gradient Descent & Optimization**  
- üîπ [Gradient Descent in Neural Networks](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/GradientDescentInNeuralNetwork.ipynb)  
- üîπ [Vanishing Gradient Problem Explained](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/VanishingGradientProblemInANN.ipynb)  
- üîπ [Techniques to Improve ANN Performance](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/HowToImprovePerformanceOfANN.ipynb)  

---

## **6Ô∏è‚É£ Regularization & Optimization Techniques**  
- üîπ [Early Stopping Implementation](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Early_Stopping_In_ANN.ipynb)  
- üîπ [Feature Scaling & Normalization Methods](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Data_or_Feature_Scaling_Normalization_In_ANN.ipynb)  
- üîπ [Dropout Layers: Theory & Practice](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Dropout_Layers_In_ANN.ipynb)  
- üîπ [Complete Guide to Regularization](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Regularization_In_Deep_Learning.ipynb)  

---

## **7Ô∏è‚É£ ANN Implementations on Real-World Datasets**  

### **üü¶ Binary Classification**  
- üîπ [Customer Churn Prediction Using ANN](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/CustomerChurnPredictionUsingANN.ipynb)  

### **üü© Multi-Class Classification**  
- üîπ [Handwritten Digit Recognition (MNIST) Using ANN](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/MNIST_classification.ipynb)  

### **üü• Regression with ANN**  
- üîπ [Neural Network for Regression Tasks](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/ANN_For__Regression_Problem.ipynb)  




---

## **8Ô∏è‚É£ Weight Initialization Techniques**  
- üîπ [Weight Initialization Pitfalls](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/WeightInitilizationTechnique(WhatNotToDo).ipynb)  
- üîπ [Xavier/Glorot & He Initialization](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Xavier_Glorat_And_He_Weight_Initialization_.ipynb)  
- üîπ [Batch Normalization Implementation](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/BatchNormalization_.ipynb)  
- üîπ [Hyperparameter Tuning with KerasTuner](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Keras_Hyperparameter_Tunning.ipynb)  

---

## **9Ô∏è‚É£üîß Optimizers**  
- üîπ [Optimizers Fundamentals](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Optimizers_.ipynb)  
- üîπ [Exponentially Weighted Moving Average](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/ExponentiallyWeightedMovingAverage_.ipynb)  
- üîπ [SGD with Momentum](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/SGD_with_Momentum_(Optimizers_Part_2).ipynb)  
- üîπ [Nesterov Accelerated Gradient](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/NesterovAcceleratedGradient(NAG)_Optimizers_Part_3.ipynb)  
- üîπ [Keras: SGD/Momentum/NAG](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Stochastic_Gradient_Descent_Imlementation_Optimizers_Part_3.ipynb)  
- üîπ [Adagrad Optimizer](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/AdaGrad_Optimizer.ipynb)  
- üîπ [RMSprop Implementation](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/RMSProp_Optimizer.ipynb)  
- üîπ [Adam Optimizer Explained](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Adam_Optimizer.ipynb)  

---

## **üîü Convolutional Neural Networks(CNN)-**
- üîπ [CNN - Intuition with Human Visual Cortex](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/ConvolutionalNeuralNetwork(CNN).ipynb)
- üîπ [CNN - Convolution Operation](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/CNN_Convolution_Operation.ipynb)
- üîπ [CNN - Padding  & Strides](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/CNN_Padding_%26_Strides_.ipynb)
- üîπ [CNN - Pooling Layer](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/CNN_Pooling_Layer.ipynb)
- üîπ [CNN - LeNet Model](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/CNN_Architecture(LeNet).ipynb)
- üîπ [CNN VS ANN](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/CNN_vs_ANN.ipynb)
- üîπ [CNN BackPropagation](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/CNN_vs_ANN.ipynb)
-  **üê∂üê± CNN for Image Classification**  
  üîπ [Dog vs. Cat Classification Using CNN](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Dog_VS_Cat_Classification_Using_CNN.ipynb)
- üîπ [CNN - Image Data Augmentation ](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/DataAugmentation.ipynb)
- üîπ [CNN - AlexNet Model and Pretrained Model](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/PreTrainedModelInCNN.ipynb)
- üîπ [CNN - Filters and Feature Map Visualization](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/VisualizingCNNFilters%26FeatureMaps.ipynb)
- üîπ [CNN - Transfer Learning In Keras](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/TransferLearningInKeras.ipynb)
- [CNN - Building Non-Linear CNN Models with Keras Functional API](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Keras_Non_Linear_Neural_Networks.ipynb)

### **‚ú® How to Use This Repository**  
1Ô∏è‚É£ Browse topics by category  
2Ô∏è‚É£ Click notebook links to view implementations  
3Ô∏è‚É£ Run notebooks in Google Colab or Jupyter  
4Ô∏è‚É£ Experiment with different parameters  

**Happy Learning!** üöÄ
