# **üß† DEEP LEARNING WORK**  

This repository contains implementations, tutorials, and resources covering fundamental to advanced deep learning concepts.

---

## **1Ô∏è‚É£ Introduction to Deep Learning**  
- üîπ [Deep Learning Basics](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/DeepLearningIntro.txt)  
- üîπ [Neuron & Neural Network Fundamentals](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Neuron%26NeuralNetwork.ipynb)  

---

## **2Ô∏è‚É£ Perceptron & Multi-Layer Perceptron (MLP)**  
- üîπ [Perceptron Implementation from Scratch](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/PerceptronImplementation.ipynb)  
- üîπ [Training: Finding Optimal Weights ('w') and Bias ('b')](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/PerceptronTraining%26Finding'w'%26'b'ValuesInPerceptron.ipynb)  
- üîπ [MLP Weight & Bias Notation Explained](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/MultilayerPerceptronNotation.ipynb)  
- üîπ [Complete MLP Training Process](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/MultilayerPerceptron.ipynb)  

---

## **3Ô∏è‚É£ Forward & Backpropagation**  
- üîπ [Step-by-Step Forward Propagation](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/ForwardPropogation.ipynb)  
- üîπ [Backpropagation: Weight Update Mechanics](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Backpropogation.ipynb)  
- üîπ [Manual Backpropagation Implementation (No Keras)](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Backpropogation_Implementation.ipynb)  

---

## **4Ô∏è‚É£ Activation & Loss Functions**  
### **Activation Functions**  
- üîπ [Intuition Behind Activation Functions](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/ActivationFunction.ipynb)  
- üîπ [Detailed Analysis of Activation Functions](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Activation_Functions_In_Deep_Learning.ipynb)  
- üîπ [Dying ReLU Problem and Solutions](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/ReLU_Problem_and_Its_Varients.ipynb)  

### **Loss Functions**  
- üîπ [Understanding Loss Functions](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/LossFunctionIntuation.ipynb)  
- üîπ [Comprehensive Guide to Loss Functions](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/LossFunctonsInNeuralNetwork.ipynb)  

---

## **5Ô∏è‚É£ Gradient Descent & Optimization**  
- üîπ [Gradient Descent in Neural Networks](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/GradientDescentInNeuralNetwork.ipynb)  
- üîπ [Vanishing Gradient Problem Explained](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/VanishingGradientProblemInANN.ipynb)  
- üîπ [Techniques to Improve ANN Performance](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/HowToImprovePerformanceOfANN.ipynb)  

---

## **6Ô∏è‚É£ Regularization & Optimization Techniques**  
- üîπ [Early Stopping Implementation](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Early_Stopping_In_ANN.ipynb)  
- üîπ [Feature Scaling & Normalization Methods](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Data_or_Feature_Scaling_Normalization_In_ANN.ipynb)  
- üîπ [Dropout Layers: Theory & Practice](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Dropout_Layers_In_ANN.ipynb)  
- üîπ [Complete Guide to Regularization](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Regularization_In_Deep_Learning.ipynb)  
---

## **7Ô∏è‚É£ Weight Initialization Techniques**  
- üîπ [Weight Initialization Pitfalls](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/WeightInitilizationTechnique(WhatNotToDo).ipynb)  
- üîπ [Xavier/Glorot & He Initialization](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Xavier_Glorat_And_He_Weight_Initialization_.ipynb)  
- üîπ [Batch Normalization Implementation](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/BatchNormalization_.ipynb)  
- üîπ [Hyperparameter Tuning with KerasTuner](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Keras_Hyperparameter_Tunning.ipynb)  

---

## **8Ô∏è‚É£üîß Optimizers**  
- üîπ [Optimizers Fundamentals](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Optimizers_.ipynb)  
- üîπ [Exponentially Weighted Moving Average](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/ExponentiallyWeightedMovingAverage_.ipynb)  
- üîπ [SGD with Momentum](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/SGD_with_Momentum_(Optimizers_Part_2).ipynb)  
- üîπ [Nesterov Accelerated Gradient](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/NesterovAcceleratedGradient(NAG)_Optimizers_Part_3.ipynb)  
- üîπ [Keras: SGD/Momentum/NAG](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Stochastic_Gradient_Descent_Imlementation_Optimizers_Part_3.ipynb)  
- üîπ [Adagrad Optimizer](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/AdaGrad_Optimizer.ipynb)  
- üîπ [RMSprop Implementation](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/RMSProp_Optimizer.ipynb)  
- üîπ [Adam Optimizer Explained](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Adam_Optimizer.ipynb)  

---

## **9Ô∏è‚É£ Convolutional Neural Networks(CNN)-**
- üîπ [CNN - Intuition with Human Visual Cortex](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/ConvolutionalNeuralNetwork(CNN).ipynb)
- üîπ [CNN - Convolution Operation](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/CNN_Convolution_Operation.ipynb)
- üîπ [CNN - Padding  & Strides](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/CNN_Padding_%26_Strides_.ipynb)
- üîπ [CNN - Pooling Layer](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/CNN_Pooling_Layer.ipynb)
- üîπ [CNN - LeNet Model](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/CNN_Architecture(LeNet).ipynb)
- üîπ [CNN VS ANN](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/CNN_vs_ANN.ipynb)
- üîπ [CNN BackPropagation](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/CNN_vs_ANN.ipynb)
-  **üê∂üê± CNN for Image Classification**  
  üîπ [Dog vs. Cat Classification Using CNN](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Dog_VS_Cat_Classification_Using_CNN.ipynb)  </br>
  üîπ [Alternative Solution of Dog vs Cat Classification Using CNN](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/DeepCNNImageClassifier_WithAnyImageipynb.ipynb)
- üîπ [CNN - Image Data Augmentation ](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/DataAugmentation.ipynb)
- üîπ [CNN - AlexNet Model and Pretrained Model](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/PreTrainedModelInCNN.ipynb)
- üîπ [CNN - Filters and Feature Map Visualization](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/VisualizingCNNFilters%26FeatureMaps.ipynb)
- üîπ [CNN - Transfer Learning In Keras](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/TransferLearningInKeras.ipynb)
- üîπ [CNN - Building Non-Linear CNN Models with Keras Functional API](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Keras_Non_Linear_Neural_Networks.ipynb)

## **1Ô∏è0Ô∏è‚É£Recurrent Neural Networks(RNN)-**
- üîπ [RNN - Basics](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Recurrent_Neural_NEtwork.ipynb)
- üîπ [RNN - Architecture & Forward Propagation ](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/RNN_Forward_Propagation.ipynb)
- üîπ [RNN - Sentiment Analysis Using Integer Encoding](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/RNN_Sentiment_Analysis_with_Keras_Code.ipynb)
- üîπ [RNN - Sentiment Analysis Using Embeddings](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/RNN_Sentiment_Analysis_Using_Embeddings.ipynb)
- üîπ [RNN - Different Types](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/RNN_Different_Types.ipynb)
- üîπ [RNN - Backpropagation](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/RNN_BackPropagation.ipynb)
- üîπ [RNN - Prolems with RNN and Solution](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/RNN_Problems_With_RNN.ipynb)

  
## **1Ô∏è1Ô∏è‚É£Transfer Learning on Imagenet Pretrained Models For Multiclass Image Classification Problem (CNN)-**
Checkout Detailed Result -  [Here](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/resultoftransferlearningonimagenetpretrainedmodelsonflowersdataset.txt)

**Model 1-** [Resnet50 model with transfer learning for Flowers classification](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/TransferLearningResNetOnFlowersDatasetipynb.ipynb)
  - Dataset is loaded without using imagedatagenerator
  - used 25 number of epochs
  - Got output as accuracy: **0.9597 - loss: 0.6167 - val_accuracy: 0.8692 - val_loss: 0.9964**
  - While making the prediction, download any one of the image from available class , upload it to google collab and copy & paste link to "image = cv2.imread('here')

**Model 2-** [Xception model with transfer learning for Flowers classification](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/TransferLearningXceptionOnFlowersDatasetipynb.ipynb)
  - Got output as accuracy: **0.7782 - loss: 1.3080 - val_accuracy: 0.5123 - val_loss: 2.2271**
    which is not at all good model. Got 51% accuracy on validation data means underfitting is there in the model.

**Model 3-**[VGG16 model with Transfer Learning for Flowers classification](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/TransferLearningVGG16OnFlowersDatasetipynb.ipynb)
 - Got output as **accuracy: 0.9192 - loss: 0.7011 - val_accuracy: 0.8474 - val_loss: 0.9967**
   which is better working than Xception model but VGG16 is not working better than ResNet50 model.

**Model 4-**[InceptionV3 model with Transfer Learning For Flowers classification](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/TransferLearningInceptionV3OnFlowersDatasetipynb.ipynb)
- Got output as **accuracy: 0.6325 - loss: 1.4828 - val_accuracy: 0.4659 - val_loss: 1.9684** which is not at all a good model to predict as we got validation accuracy less than 50%.

## **1Ô∏è3Ô∏è‚É£ Long Short Term Memory(LSTM) & Gated Recurrent Units(GRU's)-**
- [LSTM - Basics](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/LSTM(LongShortTermMemory)_Basics.ipynb)

- [LSTM - Architecture](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/LSTM_Architecture.ipynb)

- [Next Word Predictor Using LSTM](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/NextWordPredictorUsingLSTM.ipynb)
- **[Gated Recurrent Unit](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/GatedRecurrentUnit(GRU).ipynb)**

## **1Ô∏è2Ô∏è‚É£ - Bidirectional & Stacked RNN/LSTM/GRU**
- [Deep/Stacked RNN/LSTM/GRU's](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/DeepRNN.ipynb)
- [Bidirectional RNN/LSTM/GRU](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/BidirectionalRNN.ipynb)
  

## ü§ñüìöüß†üí¨ [Large Language Models and ChatGPT](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/LargeLanguageModels(LLM's).ipynb)


## **1Ô∏è3Ô∏è‚É£ - Sequence to Sequence Models**
- [Encoder - Decoder Architecture](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/SequenceToSequenceModels.ipynb)
- [Attention Mechanism](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/AttentionMechanism.ipynb)
- [Bahdanau and Luong Attention](https://github.com/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/BahdanauAttentionVSLuongAttention.ipynb)

## **1Ô∏è4Ô∏è‚É£ - Transformer Models**

### **‚ú® How to Use This Repository**  
1Ô∏è‚É£ Browse topics by category  
2Ô∏è‚É£ Click notebook links to view implementations  
3Ô∏è‚É£ Run notebooks in Google Colab or Jupyter  
4Ô∏è‚É£ Experiment with different parameters  

**Happy Learning!** üöÄ
