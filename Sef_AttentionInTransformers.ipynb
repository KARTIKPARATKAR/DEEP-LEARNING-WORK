{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMW6FmcaQjp+gs9V7+5XFds",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KARTIKPARATKAR/DEEP-LEARNING-WORK/blob/main/Sef_AttentionInTransformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is Self Attention In Attention?**\n",
        "\n",
        "To create any NLP application what is the most imortant requirement?\n",
        "\n",
        "Answer is that how we are going to convert words into numbers or vectors.We use one-hot-encoding technique to convert text into vectors.This technique is kind of inefficient due to avilability of more zeroes in vectors.\n",
        "\n",
        "To overcome this issue , we started using bag-of-words technique to represent text in numbers.\n",
        "\n",
        "Then we have tfidf technique to represent the input text in numbers.\n",
        "\n",
        "**Word-Embeddings-**\n",
        "\n",
        "The best technique to represent the words into numbers with the use of **Word-Embeddings** because this technique has the capability of capturing the semantic meaninig.This means that word-embedding technique can understand the meaning and in which context the perticular word has been used in the sentence. Word-Embedding technique converts the input word into vectors such that two similar words will have a kind of similar vector values and two dissimilar words will have a kind f dissimilar vector values."
      ],
      "metadata": {
        "id": "gWRpdAWp6LRY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem With Word-Embedding Technique-**\n",
        "\n",
        "Word-Embedding technique captures the average meanining of the word present in sentence.Let we have a dataset where Apple word is used numerous times.IN some context , Apple word is used in the context of fruit and in some context Apple word is used as a mobile manufacturing company.LEt the word Apple is being used in the dataset for 9000 times as fruit and 1000 times as a mobile manufacturing company. Let we are representing the input words in the form of 2D vectore with taste and technolgy value. [taste , technolgy] . As Apple word is being used more times as fruit than the technology, the representation of word Apple in vector with the help of word-embedding technique would look something like this -- [taste , technolgy] ---> [0.9 , 0.3]\n",
        "\n",
        "The problem here is that , word-embedding vector of the input words are created once and these vectors will be used multiple times.The word-embedding vector of word is static and this static nature is problamatic for us.\n",
        "\n",
        "Let we have a language translation problem of English to Hindi. We have a ENglish sentence like \"Apple launched a new phone while I was eating an orange\".Here word Apple is used in the sentence and the vector representation of word Apple is [taste , technolgy] ---> [0.9 , 0.3].But here word Apple is used in the contect of Technology and therefore the technology value in vectore should be more than the taste value. So here in such cases,word-embedding technique fails to work properly because word-embedding created vector representation of the words are static and we can not change the vector values according to the context of that word in which it is being used.\n",
        "\n",
        "So in such cases we require a mechanism or technique which can change the vector representation values dynamically according to the context of the word in which it is being used.\n",
        "\n",
        "This problem is being solved by self-embedding.It is a technique which\n",
        "generates the smart contextual embedding from the word-embedding vector. Self embedding can be considered as a function where we pass all the input words embedding vector and we get a one output embeddings but these output embeddings are dynamic embeddings which understands that any perticular word is being used in which context and change the vector values according to the context of word used."
      ],
      "metadata": {
        "id": "wEA_qiS29Rhk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Self-Attention One line Summary would be \"It is a mechanism that takes word-embedding/static-embedding as a input and can generate good contextual embeddings which can be used for any NLP applications.\""
      ],
      "metadata": {
        "id": "60nFWZ4J9MiT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "adhJajSZEwBt"
      }
    }
  ]
}